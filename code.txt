<!-- Dependencias de Hadoop Cliente -->
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client-api</artifactId>
    <version>${hadoop.version}</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client-runtime</artifactId>
    <version>${hadoop.version}</version>
</dependency>
<!-- Añadir hadoop-common para clases de shutdown y otras utilidades -->
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>${hadoop.version}</version>
    <exclusions>
        <!-- Excluir Jetty de hadoop-common para evitar conflicto con el Jetty de Spark -->
        <exclusion>
            <groupId>org.eclipse.jetty</groupId>
            <artifactId>*</artifactId> <!-- Excluye todos los artefactos de Jetty traídos por hadoop-common -->
        </exclusion>
        <!-- Excluir servlet-api por si acaso, Spark trae la suya -->
        <exclusion>
            <groupId>javax.servlet</groupId>
            <artifactId>*</artifactId>
        </exclusion>
        <!-- A veces, las versiones de protobuf pueden chocar -->
        <exclusion>
            <groupId>com.google.protobuf</groupId>
            <artifactId>protobuf-java</artifactId>
        </exclusion>
        <!-- SLF4J, Spark debe manejar su propia versión -->
        <exclusion>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-reload4j</artifactId>
        </exclusion>
         <exclusion>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
        </exclusion>
    </exclusions>
</dependency>